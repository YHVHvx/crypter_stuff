[Что такое информационная энтропия?]
"Мера неопределённости или непредсказуемости информации." Используется как коэффициент эффективности хранения информации.

[Как её вычислить?]
Формула энтропии по Шенону: E = -( p[1]*log2(p[1])+p[2]*log2(p[2])+ ... +p[n]*log2(p[n]) )
E - Энтропия.
p[1] - вероятность попадания данного символа.
p[n] = n/M ;n - количество повторений данного символа, M - количество символов в сообщении.
Замечу, что n - это номер символа не в сообщении, а в алфавите, из которого составлено сообщение. Для одного символа p[n] считается 1 раз!
Например:
M=aba ;исходное сообщение
p[1]= 2/3 = 0.66, ;"а" встречается с вероятностью 0.66
p[2]= 1/3 = 0.33, "b" встречается с вероятностью 0.33
E = -( 0.66 * log2(0.66) + 0.33 * log2(0.33) ) = -[(-0.39)+(-0.528)]= 0.9183

[Как уменьшить энтропию?]
Сделать хранение информацию менее эффективным, чем больше повторений конкретного символа, по сравнению с повторениями остальных, тем лучше.
abcd - 2
abcdaaaa - 1.54
abcdaaaabbbb - 1.56
abcdaaaaaaaa - 1.20

